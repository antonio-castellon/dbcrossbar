<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Using dbcrossbar</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body class="light">
        <!-- Provide site root to javascript -->
        <script type="text/javascript">var path_to_root = "";</script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { } 
            if (theme === null || theme === undefined) { theme = 'light'; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <ol class="chapter"><li class="affix"><a href="intro.html">dbcrossbar Guide</a></li><li><a href="features.html"><strong aria-hidden="true">1.</strong> Features &amp; philosophy</a></li><li><a href="installing.html"><strong aria-hidden="true">2.</strong> Installing</a></li><li><a href="how.html"><strong aria-hidden="true">3.</strong> How it works</a></li><li><ol class="section"><li><a href="csv_interchange.html"><strong aria-hidden="true">3.1.</strong> CSV interchange format</a></li><li><a href="schema.html"><strong aria-hidden="true">3.2.</strong> Portable table schema</a></li></ol></li><li><a href="commands.html"><strong aria-hidden="true">4.</strong> Commands</a></li><li><ol class="section"><li><a href="cp.html"><strong aria-hidden="true">4.1.</strong> cp: Copying tables</a></li><li><a href="count.html"><strong aria-hidden="true">4.2.</strong> count: Counting records</a></li><li><a href="conv.html"><strong aria-hidden="true">4.3.</strong> conv: Transforming schemas</a></li></ol></li><li><a href="drivers.html"><strong aria-hidden="true">5.</strong> Drivers</a></li><li><ol class="section"><li><a href="bigml.html"><strong aria-hidden="true">5.1.</strong> BigML</a></li><li><a href="bigquery.html"><strong aria-hidden="true">5.2.</strong> BigQuery</a></li><li><a href="csv.html"><strong aria-hidden="true">5.3.</strong> CSV</a></li><li><a href="gs.html"><strong aria-hidden="true">5.4.</strong> Google Cloud Storage</a></li><li><a href="postgres.html"><strong aria-hidden="true">5.5.</strong> PostgreSQL</a></li><li><a href="redshift.html"><strong aria-hidden="true">5.6.</strong> RedShift</a></li><li><a href="s3.html"><strong aria-hidden="true">5.7.</strong> S3</a></li></ol></li><li><a href="credits.html">Credits and contributors</a></li></ol>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light <span class="default">(default)</span></button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">Using dbcrossbar</h1> 

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <a class="header" href="#dbcrossbar-guide" id="dbcrossbar-guide"><h1><code>dbcrossbar</code> Guide</h1></a>
<p><code>dbcrossbar</code> is an <a href="https://github.com/dbcrossbar/dbcrossbar">open source</a> tool that copies large, tabular datasets between many different databases and storage formats. Data can be copied from any source to any destination.</p>
<pre><code class="language-dotprocess">digraph {
    rankdir=&quot;LR&quot;;

    csv_in [label=&quot;CSV&quot;]
    csv_out [label=&quot;CSV&quot;]
    csv_in -&gt; dbcrossbar -&gt; csv_out

    postgres_in [label=&quot;PostgreSQL&quot;]
    postgres_out [label=&quot;PostgreSQL&quot;]
    postgres_in -&gt; dbcrossbar -&gt; postgres_out

    bigquery_in [label=&quot;BigQuery&quot;]
    bigquery_out [label=&quot;BigQuery&quot;]
    bigquery_in -&gt; dbcrossbar -&gt; bigquery_out

    s3_in [label=&quot;S3&quot;]
    s3_out [label=&quot;S3&quot;]
    s3_in -&gt; dbcrossbar -&gt; s3_out

    gs_in [label=&quot;Cloud Storage&quot;]
    gs_out [label=&quot;Cloud Storage&quot;]
    gs_in -&gt; dbcrossbar -&gt; gs_out

    redshift_in [label=&quot;RedShift&quot;]
    redshift_out [label=&quot;RedShift&quot;]
    redshift_in -&gt; dbcrossbar -&gt; redshift_out

    etc_in [label=&quot;...&quot;]
    etc_out [label=&quot;...&quot;]
    etc_in -&gt; dbcrossbar -&gt; etc_out
}
</code></pre>
<a class="header" href="#an-example" id="an-example"><h2>An example</h2></a>
<p>If we have a CSV file <code>my_table.csv</code> containing data:</p>
<pre><code class="language-csv">id,name,quantity
1,Blue widget,10
2,Red widget,50

</code></pre>
<p>And a file <code>my_table.sql</code> containing a table definition:</p>
<pre><code class="language-sql">CREATE TABLE my_table (
    id INT NOT NULL,
    name TEXT NOT NULL,
    quantity INT NOT NULL
);

</code></pre>
<p>Then we can use these to create a PostgreSQL table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=overwrite \
    --schema=postgres-sql:my_table.sql \
    csv:my_table.csv \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table'

</code></pre>
<p>If we want to use the data to update a table in BigQuery, we can upsert into BigQuery using the <code>id</code> column:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=upsert-on:id \
    --temporary=gs://$GS_TEMP_BUCKET \
    --temporary=bigquery:$GCLOUD_PROJECT:temp_dataset \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table' \
    bigquery:$GCLOUD_PROJECT:my_dataset.my_table

</code></pre>
<p>Notice that we don't need to specify <code>--schema</code>, because <code>dbcrossbar</code> will automatically translate the PostgreSQL column types to corresponding BigQuery types.</p>
<a class="header" href="#features--philosophy" id="features--philosophy"><h1>Features &amp; philosophy</h1></a>
<p><code>dbcrossbar</code> is designed to do a few things well. Typically, <code>dbcrossbar</code> is used for loading raw data, and for moving data back and forth between production databases and data warehouses. It supports a few core features:</p>
<ol>
<li>Copying tables.</li>
<li>Counting records in tables.</li>
<li>Converting between different table schema formats, including PostgreSQL <code>CREATE TABLE</code> statements and BigQuery schema JSON.</li>
</ol>
<p><code>dbcrossbar</code> offers a number of handy features:</p>
<ul>
<li>A single static binary on Linux, with no dependencies.</li>
<li>A stream-based architecture that limits the use of RAM and requires no temporary files. (Note that the current version relies on the Google Cloud SDK, which <em>does</em> have RAM-usage problems.)</li>
<li>Support for appending, overwriting or upserting into existing tables.</li>
<li>Support for selecting records using <code>--where</code>.</li>
</ul>
<p><code>dbcrossbar</code> also supports a rich variety of portable column types:</p>
<ul>
<li>Common types, including booleans, dates, timestamps, floats, integers, and text.</li>
<li>UUIDs.</li>
<li>JSON.</li>
<li>GeoJSON.</li>
<li>Arrays.</li>
</ul>
<a class="header" href="#non-features" id="non-features"><h2>Non-features</h2></a>
<p>The following features are explicitly excluded from <code>dbcrossbar</code>'s mission:</p>
<ul>
<li>Data cleaning and transformation.</li>
<li>Fixing invalid column names.</li>
<li>Copying multiple tables at time.</li>
<li>Automatically copying constraints, foreign keys, etc.</li>
</ul>
<p>If you need these features, then take a look at tools like <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a> and <a href="https://pgloader.io/"><code>pgloader</code></a>.</p>
<a class="header" href="#installing" id="installing"><h1>Installing</h1></a>
<p>Pre-built binaries for <code>dbcrossbar</code> are <a href="https://github.com/faradayio/dbcrossbar/releases">available on GitHub</a>. These currently include:</p>
<ol>
<li>Fully-static Linux x86_64 binaries, which should work on any modern distribution (including Alpine Linux containers).</li>
<li>MacOS X binaries.</li>
</ol>
<p>Windows binaries are not available at this time, but it may be possible to build them with a little work.</p>
<a class="header" href="#required-tools" id="required-tools"><h2>Required tools</h2></a>
<p>To use the S3 and RedShift drivers, you will need to install the <a href="https://github.com/faradayio/dbcrossbar/releases">AWS CLI tools</a>.</p>
<p>To use the BigQuery and Google Cloud Storage drivers, you will need to install the <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a> to get the <code>gsutil</code> and <code>bq</code> CLI tools, and authenticate to your Google Cloud account using <code>gcloud</code>.</p>
<p>We plan to replace these external CLI tools with native Rust libraries before the 1.0 release.</p>
<a class="header" href="#installing-using-cargo" id="installing-using-cargo"><h2>Installing using <code>cargo</code></h2></a>
<p>You can also install <code>dbcrossbar</code> using <code>cargo</code>. First, you will need to make sure you have the necessary C dependencies installed:</p>
<pre><code class="language-sh"># Ubuntu Linux (might be incomplete).
sudo apt install build-essential libssl-dev libpq-dev

# MacOS X (might be incomplete).
brew install openssl@1.1 postgresql
</code></pre>
<p>Then, you can install using <code>cargo</code>:</p>
<pre><code class="language-sh">cargo install dbcrossbar
</code></pre>
<a class="header" href="#building-from-source" id="building-from-source"><h2>Building from source</h2></a>
<p>The source code is available <a href="https://github.com/faradayio/dbcrossbar">on GitHub</a>. First, install the build dependencies as described above. Then run:</p>
<pre><code class="language-sh">git clone https://github.com/faradayio/dbcrossbar.git
cd dbcrossbar
cargo build --release
</code></pre>
<p>This will create <code>target/release/dbcrossbar</code>.</p>
<a class="header" href="#how-it-works" id="how-it-works"><h1>How it works</h1></a>
<p><code>dbcrossbar</code> uses pluggable input and output drivers, allowing any input to be copied to any output:</p>
<pre><code class="language-dotprocess">digraph {
    rankdir=&quot;LR&quot;;

    csv_in [label=&quot;CSV&quot;]
    csv_out [label=&quot;CSV&quot;]
    csv_in -&gt; dbcrossbar -&gt; csv_out

    postgres_in [label=&quot;PostgreSQL&quot;]
    postgres_out [label=&quot;PostgreSQL&quot;]
    postgres_in -&gt; dbcrossbar -&gt; postgres_out

    bigquery_in [label=&quot;BigQuery&quot;]
    bigquery_out [label=&quot;BigQuery&quot;]
    bigquery_in -&gt; dbcrossbar -&gt; bigquery_out

    s3_in [label=&quot;S3&quot;]
    s3_out [label=&quot;S3&quot;]
    s3_in -&gt; dbcrossbar -&gt; s3_out

    etc_in [label=&quot;...&quot;]
    etc_out [label=&quot;...&quot;]
    etc_in -&gt; dbcrossbar -&gt; etc_out
}

</code></pre>
<a class="header" href="#parallel-data-streams" id="parallel-data-streams"><h2>Parallel data streams</h2></a>
<p>Internally, <code>dbcrossbar</code> uses parallel data streams. If we copy <code>s3://example/</code> to <code>csv:out/</code> using <code>--max-streams=4</code>, this will run up to 4 copies in parallel:</p>
<pre><code class="language-dotprocess">digraph {
    rankdir=&quot;LR&quot;;

    src1 [label=&quot;s3://example/file_1.csv&quot;]
    dest1 [label=&quot;csv:out/file_1.csv&quot;]
    src1 -&gt; dest1

    src2 [label=&quot;s3://example/file_2.csv&quot;]
    dest2 [label=&quot;csv:out/file_2.csv&quot;]
    src2 -&gt; dest2

    src3 [label=&quot;s3://example/file_3.csv&quot;]
    dest3 [label=&quot;csv:out/file_3.csv&quot;]
    src3 -&gt; dest3

    dest4 [label=&quot;csv:out/file_4.csv&quot;]
    src4 [label=&quot;s3://example/file_4.csv&quot;]
    src4 -&gt; dest4

    {
        rank=same;
        rankdir=&quot;TB&quot;;
        src1 -&gt; src2 -&gt; src3 -&gt; src4 [style=&quot;invis&quot;]
    }
}
</code></pre>
<p>As soon as one stream finishes, a new one will be started:</p>
<pre><code class="language-dotprocess">digraph {
    rankdir=&quot;LR&quot;;

    src5 [label=&quot;s3://example/file_5.csv&quot;]
    dest5 [label=&quot;csv:out/file_5.csv&quot;]
    src5 -&gt; dest5
}
</code></pre>
<p><code>dbcrossbar</code> accomplishes this using a <strong>stream of CSV streams.</strong> This allows us to make extensive use of <a href="https://ferd.ca/queues-don-t-fix-overload.html">backpressure</a> to control how data flows through the system, eliminating the need for temporary files. This makes it easier to work with 100GB+ CSV files and 1TB+ datasets.</p>
<a class="header" href="#shortcuts" id="shortcuts"><h2>Shortcuts</h2></a>
<p>When copying between certain drivers, <code>dbcrossbar</code> supports &quot;shortcuts.&quot; For example, it can load data directly from Google Cloud Storage into BigQuery.</p>
<a class="header" href="#multi-threaded-asynchronous-rust" id="multi-threaded-asynchronous-rust"><h2>Multi-threaded, asynchronous Rust</h2></a>
<p><code>dbcrossbar</code> is written using <a href="https://rust-lang.github.io/async-book/">asynchronous</a> <a href="https://www.rust-lang.org/">Rust</a>, and it makes heavy use of a multi-threaded worker pool. Internally, it works something like a set of classic Unix pipelines running in parallel. Thanks to Rust, it bas been possible to get native performance and multithreading without spending too much time debugging.</p>
<a class="header" href="#csv-interchange-format" id="csv-interchange-format"><h1>CSV interchange format</h1></a>
<p>Internally, <code>dbcrossbar</code> converts all data into CSV streams. For many standard types, all input drivers are required to provide byte-for-byte identical CSV data:</p>
<pre><code class="language-csv">id,test_bool,test_date,test_int16,test_int32,test_int64,test_text,test_timestamp_without_time_zone,test_timestamp_with_time_zone,test_uuid,testCapitalized
1,t,1969-07-20,-32768,-2147483648,-9223372036854775808,hello,1969-07-20T20:17:39,1969-07-20T20:17:39Z,084ec3bb-3193-4ffb-8b74-99a288e8432c,
2,f,2001-01-01,32767,2147483647,9223372036854775807,,,,,
3,,,,,,,,,,
</code></pre>
<p>For more complex types such as arrays, JSON columns and GeoJSON data, we embed JSON into the CSV file:</p>
<pre><code class="language-csv">test_null,test_not_null,test_bool,test_bool_array,test_date,test_date_array,test_float32,test_float32_array,test_float64,test_float64_array,test_geojson,test_geojson_3857,test_int16,test_int16_array,test_int32,test_int32_array,test_int64,test_int64_array,test_json,test_text,test_text_array,test_timestamp_without_time_zone,test_timestamp_without_time_zone_array,test_timestamp_with_time_zone,test_timestamp_with_time_zone_array,test_uuid,test_uuid_array
,hi,t,&quot;[true,false]&quot;,1969-07-20,&quot;[&quot;&quot;1969-07-20&quot;&quot;]&quot;,1e+37,&quot;[1e-37,0,100.125,1e+37]&quot;,1e+37,&quot;[1e-37,0,1000.125,1e+37]&quot;,&quot;{&quot;&quot;type&quot;&quot;:&quot;&quot;Point&quot;&quot;,&quot;&quot;coordinates&quot;&quot;:[-71,42]}&quot;,&quot;{&quot;&quot;type&quot;&quot;:&quot;&quot;Point&quot;&quot;,&quot;&quot;coordinates&quot;&quot;:[-71,42]}&quot;,16,&quot;[-32768,0,32767]&quot;,32,&quot;[-2147483648,0,2147483647]&quot;,64,&quot;[&quot;&quot;-9223372036854775808&quot;&quot;,&quot;&quot;0&quot;&quot;,&quot;&quot;9223372036854775807&quot;&quot;]&quot;,&quot;{&quot;&quot;x&quot;&quot;: 1, &quot;&quot;y&quot;&quot;: 2}&quot;,hello,&quot;[&quot;&quot;hello&quot;&quot;,&quot;&quot;&quot;&quot;]&quot;,1969-07-20T20:17:39.5,&quot;[&quot;&quot;1969-07-20T20:17:39.5&quot;&quot;]&quot;,1969-07-20T20:17:39.5Z,&quot;[&quot;&quot;1969-07-20T20:17:39.5Z&quot;&quot;]&quot;,084ec3bb-3193-4ffb-8b74-99a288e8432c,&quot;[&quot;&quot;084ec3bb-3193-4ffb-8b74-99a288e8432c&quot;&quot;]&quot;

</code></pre>
<a class="header" href="#tricks-for-preparing-csv-data" id="tricks-for-preparing-csv-data"><h2>Tricks for preparing CSV data</h2></a>
<p>If your input CSV files use an incompatible format, there are several things that might help. If your CSV files are invalid, non-standard, or full of junk, then you may be able to use <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a> or <a href="https://github.com/BurntSushi/xsv"><code>xsv</code></a> to fix the worst problems.</p>
<p>If you need to clean up your data manually, then you may want to consider using <code>dbcrossbar</code> to load your data into BigQuery, and set your columns to type <code>STRING</code>. Once this is done, you can parse and normalize your data quickly using SQL queries.</p>
<a class="header" href="#portable-table-schema" id="portable-table-schema"><h1>Portable table schema</h1></a>
<p>Internally, <code>dbcrossbar</code> uses a portable table &quot;schema&quot; format. This provides a common ground between PostgreSQL's <code>CREATE TABLE</code> statements, <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery's JSON schemas</a>, and equivalent formats for other databases. Right now, this is only documented in the library documentation:</p>
<ul>
<li><a href="https://docs.rs/dbcrossbarlib/latest/dbcrossbarlib/schema/index.html">The <code>dbcrossbar</code> schema format</a>.</li>
<li><a href="https://docs.rs/dbcrossbarlib/latest/dbcrossbarlib/schema/enum.DataType.html">The <code>dbcrossbar</code> column types</a>.</li>
</ul>
<p>All table schemas and column types are converted into the portable format and then into the appropriate destination format.</p>
<a class="header" href="#commands" id="commands"><h1>Commands</h1></a>
<p><code>dbcrossbar</code> supports four main subcommands:</p>
<ul>
<li><code>dbcrossbar cp</code>: Copy tabular data.</li>
<li><code>dbcrossbar count</code>: Count records.</li>
<li><code>dbcrossbar conv</code>: Convert table schemas between databases.</li>
</ul>
<p>For more information, type <code>dbcrossbar --help</code> or <code>dbcrossbar $CMD --help</code>.</p>
<p>Not all drivers support all the features of each command. To see the available drivers and what commands they support, run <code>dbcrossbar features</code> and <code>dbcrossbar features $DRIVER_NAME</code>.</p>
<a class="header" href="#cp-copying-tables" id="cp-copying-tables"><h1><code>cp</code>: Copying tables</h1></a>
<p>The <code>cp</code> command copies tabular data from a source location to a destination location. For example, we can copy a CSV file into PostgreSQL, replacing any existing table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=overwrite \
    --schema=postgres-sql:my_table.sql \
    csv:my_table.csv \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table'

</code></pre>
<p>Or we copy data from PostgreSQL and upsert it into a BigQuery table:</p>
<pre><code class="language-sh">dbcrossbar cp \
    --if-exists=upsert-on:id \
    --temporary=gs://$GS_TEMP_BUCKET \
    --temporary=bigquery:$GCLOUD_PROJECT:temp_dataset \
    'postgres://postgres@127.0.0.1:5432/postgres#my_table' \
    bigquery:$GCLOUD_PROJECT:my_dataset.my_table

</code></pre>
<a class="header" href="#command-line-help" id="command-line-help"><h2>Command-line help</h2></a>
<pre><code class="language-txt">dbcrossbar-cp 0.3.0
Copy tables from one location to another

USAGE:
    dbcrossbar cp [FLAGS] [OPTIONS] &lt;from-locator&gt; &lt;to-locator&gt;

FLAGS:
        --display-output-locators
            Display where we wrote our output data

    -h, --help                       Prints help information
    -V, --version                    Prints version information

OPTIONS:
        --from-arg &lt;from-args&gt;...
            Pass an extra argument of the form `key=value` to the
            source driver
        --if-exists &lt;if-exists&gt;
            One of `error`, `overwrite`, `append` or `upsert-on:COL`
            [default: error]
    -J, --max-streams &lt;max-streams&gt;
            How many data streams should we attempt to copy in
            parallel? [default: 4]
        --schema &lt;schema&gt;
            The schema to use (defaults to input table schema)

        --stream-size &lt;stream-size&gt;
            Specify the approximate size of the CSV streams
            manipulated by `dbcrossbar`. This can be used to split a
            large input into multiple smaller outputs. Actual data
            streams may be bigger or smaller depending on a number of
            factors. Examples: &quot;100000&quot;, &quot;1Gb&quot;
        --temporary &lt;temporaries&gt;...
            Temporary directories, cloud storage buckets, datasets to
            use during transfer (can be repeated)
        --to-arg &lt;to-args&gt;...
            Pass an extra argument of the form `key=value` to the
            destination driver
        --where &lt;where-clause&gt;
            SQL where clause specifying rows to use


ARGS:
    &lt;from-locator&gt;    The input table
    &lt;to-locator&gt;      The output table

EXAMPLE LOCATORS:
    postgres://localhost:5432/db#table
    bigquery:project:dataset.table

</code></pre>
<a class="header" href="#flags" id="flags"><h2>Flags</h2></a>
<p>Not all command-line options are supported by all drivers. See the chapter on each driver for details.</p>
<a class="header" href="#a--where" id="a--where"><h3><code>--where</code></h3></a>
<p>Specify a <code>WHERE</code> clause to include in the SQL query. This can be used to select a subset of the source rows.</p>
<a class="header" href="#a--from-arg" id="a--from-arg"><h3><code>--from-arg</code></h3></a>
<p>This can be used to specify driver-specific options for the source driver. See the chapter for that driver.</p>
<a class="header" href="#a--if-existserror" id="a--if-existserror"><h3><code>--if-exists=error</code></h3></a>
<p>If the destination location already contains data, exit with an error.</p>
<a class="header" href="#a--if-existsappend" id="a--if-existsappend"><h3><code>--if-exists=append</code></h3></a>
<p>If the destination location already contains data, append the new data.</p>
<a class="header" href="#a--if-existsoverwrite" id="a--if-existsoverwrite"><h3><code>--if-exists=overwrite</code></h3></a>
<p>If the destination location already contains data, replace it with the new data.</p>
<a class="header" href="#a--if-existsupset-oncol1" id="a--if-existsupset-oncol1"><h3><code>--if-exists=upset-on:COL1,..</code></h3></a>
<p>For every row in the new data:</p>
<ul>
<li>If a row with a matching <code>col1</code>, <code>col2</code>, etc., exists, use the new data to update that row.</li>
<li>If no row matching <code>col1</code>, <code>col2</code>, etc., exists, then insert the new row.</li>
</ul>
<p>The columns <code>col1</code>, <code>col2</code>, etc., must be marked as <code>NOT NULL</code>.</p>
<a class="header" href="#a--schema" id="a--schema"><h3><code>--schema</code></h3></a>
<p>By default, <code>dbcrossbar</code> will use the schema of the source table. But when this can't be inferred automatically, <code>--schema</code> can be used to specify a table schema:</p>
<ul>
<li><code>--schema=postgres-sql:my_table.sql</code>: A PostgreSQL <code>CREATE TABLE</code> statement.</li>
<li><code>--schema=bigquery-schema:my_table.json</code>: A <a href="https://cloud.google.com/bigquery/docs/schemas">BigQuery JSON schema</a>.</li>
<li><code>--schema=dbcrossbar-schema:my_table.json</code>: An <a href="./schema.html">internal <code>dbcrossbar</code> schema</a>.</li>
</ul>
<p>It's also possible to use a schema from an existing database table:</p>
<ul>
<li><code>--schema=postgres://localhost:5432/db#table</code></li>
<li><code>--schema=bigquery:project:dataset.table</code></li>
</ul>
<p>Note that it's possible to create a BigQuery table using a PostgreSQL schema, or vice versa. Internally, all schemes are first converted to the <a href="./schema.html">internal schema format</a>.</p>
<a class="header" href="#a--temporary" id="a--temporary"><h3><code>--temporary</code></h3></a>
<p>Specify temporary storage, which is required by certain drivers. Typical values include:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code></li>
<li><code>--temporary=gs://$GS_TEMP_BUCKET</code></li>
<li><code>--temporary=bigquery:$GCLOUD_PROJECT:temp_dataset</code></li>
</ul>
<a class="header" href="#a--to-arg" id="a--to-arg"><h3><code>--to-arg</code></h3></a>
<p>This can be used to specify driver-specific options for the destination driver. See the chapter for that driver.</p>
<a class="header" href="#count-counting-records" id="count-counting-records"><h1>count: Counting records</h1></a>
<p>This command mostly works like the <a href="./cp.html"><code>cp</code></a> command, except that it prints out a number of rows. Check your driver to see if it supports <code>count</code>.</p>
<a class="header" href="#command-line-help-1" id="command-line-help-1"><h2>Command-line help</h2></a>
<pre><code class="language-txt">dbcrossbar-count 0.3.0
Count records

USAGE:
    dbcrossbar count [OPTIONS] &lt;locator&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
        --from-arg &lt;from-args&gt;...
            Pass an extra argument of the form `key=value` to the
            source driver
        --schema &lt;schema&gt;
            The schema to use (defaults to input table schema)

        --temporary &lt;temporaries&gt;...
            Temporary directories, cloud storage buckets, datasets to
            use during transfer (can be repeated)
        --where &lt;where-clause&gt;
            SQL where clause specifying rows to use


ARGS:
    &lt;locator&gt;    The locator specifying the records to count

EXAMPLE LOCATORS:
    postgres://localhost:5432/db#table
    bigquery:project:dataset.table

</code></pre>
<a class="header" href="#conv-transforming-schemas" id="conv-transforming-schemas"><h1><code>conv</code>: Transforming schemas</h1></a>
<p>The <code>conv</code> command can be used to convert between different database schemas. To convert from a PostgreSQL <code>CREATE TABLE</code> statement to a BigQuery schema, run:</p>
<pre><code class="language-sh">dbcrossbar conv postgres-sql:table.sql bigquery-schema:table.json
</code></pre>
<p>As a handy trick, you can also use a CSV source, which will generate a <code>CREATE TABLE</code> where all columns have the type <code>TEXT</code>:</p>
<pre><code class="language-sh">dbcrossbar conv csv:data.csv postgres-sql:table.sql
</code></pre>
<p>This can then be edited to specify appropriate column types.</p>
<a class="header" href="#command-line-help-2" id="command-line-help-2"><h2>Command-line help</h2></a>
<pre><code class="language-txt">dbcrossbar-conv 0.3.0
Convert table schemas from one format to another

USAGE:
    dbcrossbar conv [OPTIONS] &lt;from-locator&gt; &lt;to-locator&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
        --if-exists &lt;if-exists&gt;
            One of `error`, `overrwrite` or `append` [default: error]


ARGS:
    &lt;from-locator&gt;    The input schema
    &lt;to-locator&gt;      The output schema

EXAMPLE LOCATORS:
    postgres-sql:table.sql
    postgres://localhost:5432/db#table
    bigquery-schema:table.json

</code></pre>
<a class="header" href="#drivers" id="drivers"><h1>Drivers</h1></a>
<p><code>dbcrossbar</code> uses built-in &quot;drivers&quot; to read and write CSV data and table schemas.</p>
<a class="header" href="#bigml" id="bigml"><h1>BigML</h1></a>
<p><a href="https://bigml.com/">BigML</a> is a hosted machine-learning service, with support for many common algorithms and server-side batch scripts.</p>
<a class="header" href="#example-locators" id="example-locators"><h2>Example locators</h2></a>
<p>Source locators:</p>
<ul>
<li><code>bigml:dataset/$ID</code>: Read data from a BigML dataset.</li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>bigml:source</code>: Create a single BigML &quot;source&quot; resource from the input data.</li>
<li><code>bigml:sources</code>: Create multiple BigML &quot;source&quot; resources from the input data.</li>
<li><code>bigml:dataset</code>: Create a single BigML &quot;source&quot; resource from the input data.</li>
<li><code>bigml:datasets</code>: Create multiple BigML &quot;source&quot; resources from the input data.</li>
</ul>
<p>If you use BigML as a destination, <code>dbcrossbar</code> will automatically activate <code>--display-output-locators</code>, and it will print locators for all the created resources on standard output. Column types on created &quot;source&quot; resources will be set something appropriate (but see <code>optype_for_text</code> below.)</p>
<a class="header" href="#configuration--authentication" id="configuration--authentication"><h2>Configuration &amp; authentication</h2></a>
<p>The BigML driver requires more configuration than most.</p>
<p>You'll need to set the following environment variables:</p>
<ul>
<li><code>BIGML_USERNAME</code>: Set this to your BigML username.</li>
<li><code>BIGML_API_KEY</code>: Set this to your BigML API key.</li>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials when using BigML as a destination. Do <strong>not</strong> set <code>AWS_SESSION_TOKEN</code>; it will not work with BigML.</li>
</ul>
<p>You'll also need to pass the following on the command line when using:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code>: Specify where to stage files for loading into BigML. This is not needed when using BigML as a source.</li>
</ul>
<p>You can also specify the following <code>--to-arg</code> values:</p>
<ul>
<li><code>name</code>: The human-readable name of the resource to create.</li>
<li><code>optype_for_text</code>: The BigML optype to use for text fields. This defaults to <code>text</code>, but you can also set it to <code>categorical</code> if your text fields contain a limited set of values.</li>
<li><code>tag</code>: This may be specified repeatedly to attach tags to the create resources.</li>
</ul>
<a class="header" href="#supported-features" id="supported-features"><h2>Supported features</h2></a>
<pre><code class="language-txt">bigml features:
- conv FROM
- cp FROM:
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col

</code></pre>
<p>Note that <code>--if-exists</code> is simply ignored, because BigML will always create new resources.</p>
<a class="header" href="#bigquery" id="bigquery"><h1>BigQuery</h1></a>
<p>Google's <a href="https://cloud.google.com/bigquery/">BigQuery</a> is a extremely scalable data warehouse that supports rich SQL queries and petabytes of data. If you need to transform or analyze huge data sets, it's an excellent tool.</p>
<p>When loading data into BigQuery, or extracting it, we always go via Google Cloud Storage. This is considerably faster than the load and extract functionality supplied by tools like <code>bq</code>.</p>
<p><strong>COMPATIBILITY WARNING:</strong> This driver currently relies on <code>gsutil</code> and <code>bq</code> for many tasks, but those tools are poorly-suited to the kind of automation we need. In particular, <code>gsutil</code> uses too much RAM, and <code>bq</code> sometimes print status messages on standard output instead of standard error. We plan to replace those tools with native Rust libraries at some point. This will change how the BigQuery driver handles authentication in a future version.</p>
<a class="header" href="#example-locators-1" id="example-locators-1"><h2>Example locators</h2></a>
<ul>
<li><code>bigquery:$PROJECT:$DATASET.$TABLE</code>: A BigQuery table.</li>
</ul>
<a class="header" href="#configuration--authentication-1" id="configuration--authentication-1"><h2>Configuration &amp; authentication</h2></a>
<p>Right now, all authentication is handled using <code>gcloud auth</code> from the <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>. <strong>This will change in a future release.</strong></p>
<p>The following command-line options will usually need to be specified for both sources and destinations:</p>
<ul>
<li><code>--temporary=gs://$GS_TEMP_BUCKET</code>: A Google Cloud Storage bucket to use for staging data in both directions.</li>
<li><code>--temporary=bigquery:$GCLOUD_PROJECT:temp_dataset</code></li>
</ul>
<a class="header" href="#supported-features-1" id="supported-features-1"><h2>Supported features</h2></a>
<pre><code class="language-txt">bigquery features:
- conv FROM
- count
  --where=$SQL_EXPR
- cp FROM:
  --where=$SQL_EXPR
- cp TO:
  --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col

</code></pre>
<a class="header" href="#csv" id="csv"><h1>CSV</h1></a>
<p><code>dbcrossbar</code> works with valid CSV files in our <a href="./csv_interchange.html">CSV interchange format</a>. For invalid CSV files, take a look at <a href="https://github.com/faradayio/scrubcsv"><code>scrubcsv</code></a>. For CSV files which need further transformation and parsing, considering loading them into BigQuery and cleaning them up using SQL. This works very well even for large datasets.</p>
<a class="header" href="#example-locators-2" id="example-locators-2"><h2>Example locators</h2></a>
<p>The following locators can be used for both input and output:</p>
<ul>
<li><code>csv:file.csv</code>: A single CSV file.</li>
<li><code>csv:dir/</code>: A directory tree containing CSV files.</li>
<li><code>csv:-</code>: Read from standard input, or write to standard output.</li>
</ul>
<p>To concatenate CSV files, use:</p>
<pre><code class="language-sh">dbcrossbar cp csv:input/ csv:merged.csv
</code></pre>
<p>To split a CSV file, use <code>--stream-size</code>:</p>
<pre><code class="language-sh">dbcrossbar cp --stream-size=&quot;100Mb&quot; csv:giant.csv csv:split/
</code></pre>
<a class="header" href="#configuration--authentication-2" id="configuration--authentication-2"><h2>Configuration &amp; authentication</h2></a>
<p>None.</p>
<a class="header" href="#supported-features-2" id="supported-features-2"><h2>Supported features</h2></a>
<pre><code class="language-txt">csv features:
- conv FROM
- cp FROM:
- cp TO:
  --if-exists=error --if-exists=overwrite

</code></pre>
<a class="header" href="#google-cloud-storage" id="google-cloud-storage"><h1>Google Cloud Storage</h1></a>
<p>Google Cloud Storage is a bucket-based storage system similar to Amazon's S3. It's frequently used in connection with BigQuery and other Google Cloud services.</p>
<p><strong>COMPATIBILITY WARNING:</strong> This driver currently relies on <code>gsutil</code> for many tasks, but <code>gsutil</code> is poorly-suited to the kind of automation we need. In particular, <code>gsutil</code> uses too much RAM, and has poor timeout behavio. We plan to replace it with native Rust libraries at some point. This will change how the Cloud Storage driver handles authentication in a future version.</p>
<a class="header" href="#example-locators-3" id="example-locators-3"><h2>Example locators</h2></a>
<p>Source locators:</p>
<ul>
<li><code>gs://bucket/dir/file.csv</code></li>
<li><code>gs://bucket/dir/</code></li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>gs://bucket/dir/</code></li>
</ul>
<p>At this point, we do not support single-file output to a cloud bucket. This is relatively easy to add, but has not yet been implemented.</p>
<a class="header" href="#configuration--authentication-3" id="configuration--authentication-3"><h2>Configuration &amp; authentication</h2></a>
<p>Right now, all authentication is handled using <code>gcloud auth</code> from the <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>. <strong>This will change in a future release.</strong></p>
<a class="header" href="#supported-features-3" id="supported-features-3"><h2>Supported features</h2></a>
<pre><code class="language-txt">gs features:
- cp FROM:
- cp TO:
  --if-exists=overwrite

</code></pre>
<a class="header" href="#postgresql" id="postgresql"><h1>PostgreSQL</h1></a>
<p><a href="https://www.postgresql.org/">PostgreSQL</a> is an excellent general-purpose SQL database.</p>
<a class="header" href="#example-locators-4" id="example-locators-4"><h2>Example locators</h2></a>
<p><code>dbcrossbar</code> supports standard PostgreSQL locators followed by <code>#table_name</code>:</p>
<ul>
<li><code>postgres://postgres:$PASSWORD@127.0.0.1:5432/postgres#my_table</code></li>
</ul>
<p>Note that PostgreSQL sources will currently output all data as a single stream. This can be split into multiple streams using the <code>--stream-size</code> option if desired.</p>
<a class="header" href="#configuration--authentication-4" id="configuration--authentication-4"><h2>Configuration &amp; authentication</h2></a>
<p>Authentication is currently handled using standard <code>postgres://user:pass@...</code> syntax, similar to <code>psql</code>. We may add alternative mechanisms at some point to avoid passing credentials on the command-line.</p>
<a class="header" href="#supported-features-4" id="supported-features-4"><h2>Supported features</h2></a>
<pre><code class="language-txt">postgres features:
- conv FROM
- count
  --where=$SQL_EXPR
- cp FROM:
  --where=$SQL_EXPR
- cp TO:
  --if-exists=error --if-exists=append --if-exists=overwrite --if-exists=upsert-on:col

</code></pre>
<a class="header" href="#redshift" id="redshift"><h1>RedShift</h1></a>
<p>Amazon's <a href="https://aws.amazon.com/redshift/">Redshift</a> is a cloud-based data warehouse designed to support analytical queries. This driver receives less testing than our BigQuery driver, because the cheapest possible RedShift test system costs over $100/month. Sponsors are welcome!</p>
<a class="header" href="#example-locators-5" id="example-locators-5"><h2>Example locators</h2></a>
<p>These are identical to <a href="./postgres.html#example-locators">PostgreSQL locators</a>, except that <code>postgres</code> is replaced by <code>redshift</code>:</p>
<ul>
<li><code>redshift://postgres:$PASSWORD@127.0.0.1:5432/postgres#my_table</code></li>
</ul>
<a class="header" href="#configuration--authentication-5" id="configuration--authentication-5"><h2>Configuration &amp; authentication</h2></a>
<p>Authentication is currently handled using the <code>redshift://user:pass@...</code> syntax. We may add alternative mechanisms at some point to avoid passing credentials on the command-line.</p>
<p>The following environment variables are required.</p>
<ul>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials.</li>
<li><code>AWS_SESSION_TOKEN</code> (optional): This should work, but it hasn't been tested.</li>
</ul>
<p>The following <code>--temporary</code> flag is required:</p>
<ul>
<li><code>--temporary=s3://$S3_TEMP_BUCKET</code>: Specify where to stage files for loading or unloading data.</li>
</ul>
<p><a href="https://docs.aws.amazon.com/redshift/latest/dg/loading-data-access-permissions.html">Authentication credentials for <code>COPY</code></a> may be passed using <code>--to-arg</code>. For example:</p>
<ul>
<li><code>--to-arg=iam_role=$ROLE</code></li>
<li><code>--to-arg=region=$REGION</code></li>
</ul>
<p>This may require some experimentation.</p>
<a class="header" href="#supported-features-5" id="supported-features-5"><h2>Supported features</h2></a>
<pre><code class="language-txt">redshift features:
- conv FROM
- cp FROM:
  --from-arg=$NAME=$VALUE --where=$SQL_EXPR
- cp TO:
  --to-arg=$NAME=$VALUE
  --if-exists=append --if-exists=overwrite

</code></pre>
<a class="header" href="#s3" id="s3"><h1>S3</h1></a>
<p>Amazon's <a href="https://aws.amazon.com/s3/">S3</a> is a bucket-based system for storing data in the cloud.</p>
<a class="header" href="#example-locators-6" id="example-locators-6"><h2>Example locators</h2></a>
<p>Source locators:</p>
<ul>
<li><code>s3://bucket/dir/file.csv</code></li>
<li><code>s3://bucket/dir/</code></li>
</ul>
<p>Destination locators:</p>
<ul>
<li><code>s3://bucket/dir/</code></li>
</ul>
<p>At this point, we do not support single-file output to a cloud bucket. This is relatively easy to add, but has not yet been implemented.</p>
<a class="header" href="#configuration--authentication-6" id="configuration--authentication-6"><h2>Configuration &amp; authentication</h2></a>
<p>The following environment variables are required:</p>
<ul>
<li><code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>: Set these to your AWS credentials.</li>
<li><code>AWS_SESSION_TOKEN</code> (optional): Set this to use temporary AWS crdentials.</li>
</ul>
<a class="header" href="#supported-features-6" id="supported-features-6"><h2>Supported features</h2></a>
<pre><code class="language-txt">s3 features:
- cp FROM:
- cp TO:
  --if-exists=overwrite

</code></pre>
<a class="header" href="#credits-and-contributors" id="credits-and-contributors"><h1>Credits and contributors</h1></a>
<p>The development of <code>dbcrossbar</code> has been generously supported by <a href="https://faraday.io/">Faraday</a>, which provides datascience and AI products for B2C companies.</p>
<p>Ongoing development of <code>dbcrossbar</code> is also supported by <a href="http://kiddsoftware.com/">Kidd Software LLC</a>, which creates custom software for businesses around the world.</p>
<a class="header" href="#contributors" id="contributors"><h2>Contributors</h2></a>
<p><code>dbcrossbar</code> is primarily maintained by Eric Kidd. Other contributors include:</p>
<ul>
<li>Bill Morris</li>
<li>Forrest Wallace</li>
<li>Prithaj Nath</li>
<li>Seamus Abshere</li>
</ul>
<p>We have also received very helpful bug reports and feature requests from our users. Thank you!</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
